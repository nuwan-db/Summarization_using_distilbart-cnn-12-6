{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9d1579-8481-46c0-89a9-94302e8f846a",
     "showTitle": true,
     "title": "Extract the dataset using sh command"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following additional packages will be installed:\n  p7zip\nSuggested packages:\n  p7zip-rar\nThe following NEW packages will be installed:\n  p7zip p7zip-full\n0 upgraded, 2 newly installed, 0 to remove and 42 not upgraded.\nNeed to get 1549 kB of archives.\nAfter this operation, 5847 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 p7zip amd64 16.02+dfsg-8 [363 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 p7zip-full amd64 16.02+dfsg-8 [1186 kB]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1549 kB in 3s (473 kB/s)\nSelecting previously unselected package p7zip.\r\n(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 100130 files and directories currently installed.)\r\nPreparing to unpack .../p7zip_16.02+dfsg-8_amd64.deb ...\r\nUnpacking p7zip (16.02+dfsg-8) ...\r\nSelecting previously unselected package p7zip-full.\r\nPreparing to unpack .../p7zip-full_16.02+dfsg-8_amd64.deb ...\r\nUnpacking p7zip-full (16.02+dfsg-8) ...\r\nSetting up p7zip (16.02+dfsg-8) ...\r\nSetting up p7zip-full (16.02+dfsg-8) ...\r\nProcessing triggers for man-db (2.10.2-1) ...\r\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0 46.1M    0  111k    0     0  65918      0  0:12:14  0:00:01  0:12:13 95558\r  6 46.1M    6 3072k    0     0  1213k      0  0:00:38  0:00:02  0:00:36 1542k\r 12 46.1M   12 6127k    0     0  1718k      0  0:00:27  0:00:03  0:00:24 2024k\r 21 46.1M   21 9951k    0     0  2233k      0  0:00:21  0:00:04  0:00:17 2540k\r 31 46.1M   31 14.4M    0     0  2707k      0  0:00:17  0:00:05  0:00:12 3003k\r 43 46.1M   43 19.9M    0     0  3127k      0  0:00:15  0:00:06  0:00:09 4238k\r 54 46.1M   54 25.3M    0     0  3477k      0  0:00:13  0:00:07  0:00:06 4641k\r 64 46.1M   64 29.6M    0     0  3585k      0  0:00:13  0:00:08  0:00:05 4944k\r 74 46.1M   74 34.5M    0     0  3743k      0  0:00:12  0:00:09  0:00:03 5090k\r 86 46.1M   86 39.9M    0     0  3883k      0  0:00:12  0:00:10  0:00:02 5156k\r 97 46.1M   97 45.0M    0     0  4031k      0  0:00:11  0:00:11 --:--:-- 5232k\r100 46.1M  100 46.1M    0     0  4034k      0  0:00:11  0:00:11 --:--:-- 5008k\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,32 CPUs AMD EPYC 7R32 (830F10),ASM,AES-NI)\n\nScanning the drive for archives:\n1 file, 48416494 bytes (47 MiB)\n\nExtracting archive: quant.7z\n--\nPath = quant.7z\nType = 7z\nPhysical Size = 48416494\nHeaders Size = 333\nMethod = BZip2\nSolid = -\nBlocks = 8\n\nEverything is Ok\n\nFiles: 8\nSize:       266808589\nCompressed: 48416494\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "#To keep it simple, we'll download and extract the dataset using standard bash commands \n",
    "#Install 7zip to extract the file\n",
    "apt-get install -y p7zip-full\n",
    "\n",
    "rm -rf /tmp/quant || true\n",
    "mkdir -p /tmp/quant\n",
    "cd /tmp/quant\n",
    "#Download & extract the quant archive\n",
    "curl -L https://archive.org/download/stackexchange/quant.stackexchange.com.7z -o quant.7z\n",
    "7z x quant.7z \n",
    "#Move the dataset to our main bucket\n",
    "rm -rf /dbfs/dbdemos/product/llm/quant/raw || true\n",
    "mkdir -p /dbfs/dbdemos/product/llm/quant/raw\n",
    "cp -f Posts.xml /dbfs/dbdemos/product/llm/quant/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2945a1-87cd-48a6-8706-a00e43de7be0",
     "showTitle": true,
     "title": "Our Q&A dataset is ready"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/dbdemos/product/llm/quant/raw/Posts.xml</td><td>Posts.xml</td><td>73662724</td><td>1687046644000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/dbdemos/product/llm/quant/raw/Posts.xml",
         "Posts.xml",
         73662724,
         1687046644000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /dbdemos/product/llm/quant/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275294c7-5bf5-47f6-96bb-14226ed026ff",
     "showTitle": true,
     "title": "Review our raw Q&A dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw xml dataset under /dbdemos/product/llm/quant/raw\n+-----------------+------------+--------------------+--------------------+-------------+--------------------+---------------+--------------------+--------------+---+--------------------+--------------------+----------------------+-----------------+-----------------+------------+---------+-----------+------+--------------------+--------------------+----------+\n|_AcceptedAnswerId|_AnswerCount|               _Body|         _ClosedDate|_CommentCount| _CommunityOwnedDate|_ContentLicense|       _CreationDate|_FavoriteCount|_Id|   _LastActivityDate|       _LastEditDate|_LastEditorDisplayName|_LastEditorUserId|_OwnerDisplayName|_OwnerUserId|_ParentId|_PostTypeId|_Score|               _Tags|              _Title|_ViewCount|\n+-----------------+------------+--------------------+--------------------+-------------+--------------------+---------------+--------------------+--------------+---+--------------------+--------------------+----------------------+-----------------+-----------------+------------+---------+-----------+------+--------------------+--------------------+----------+\n|             null|           8|<p>To get the bal...|2011-01-31 21:49:...|            8|2011-01-31 21:13:...|   CC BY-SA 2.5|2011-01-31 21:02:...|          null|  1|2011-01-31 21:39:...|2011-01-31 21:13:...|                  null|               23|             null|           6|     null|          1|    26|<learning><financ...|What are some goo...|      5651|\n|             null|        null|<p>I like Statist...|                null|            0|                null|   CC BY-SA 2.5|2011-01-31 21:05:...|          null|  2|2011-01-31 21:05:...|                null|                  null|             null|             null|          33|        1|          2|     6|                null|                null|      null|\n|             null|           5|<p>I want to star...|2011-01-31 21:52:...|            6|                null|   CC BY-SA 2.5|2011-01-31 21:07:...|          null|  3|2011-01-31 21:53:...| 2017-04-13 12:46:23|                  null|               -1|             null|          27|     null|          1|    12| <learning><finance>|What blogs or art...|      1800|\n|             null|        null|<p>John C. Hull's...|                null|            0|                null|   CC BY-SA 2.5|2011-01-31 21:08:...|          null|  4|2011-01-31 21:08:...|                null|                  null|             null|             null|          17|        1|          2|     5|                null|                null|      null|\n|              120|           2|<p>How do you mod...|                null|            0|                null|   CC BY-SA 2.5|2011-01-31 21:08:...|          null|  5|2014-07-16 15:52:...|2011-02-01 16:53:...|                  null|               40|             null|          40|     null|          1|    19|      <risk><credit>|Concentration ris...|       899|\n|             null|        null|<p>This may be to...|                null|            0|                null|   CC BY-SA 2.5|2011-01-31 21:09:...|          null|  6|2011-01-31 21:09:...|                null|                  null|             null|             null|          30|        1|          2|     5|                null|                null|      null|\n|             null|        null|<p>I like the fol...|                null|            0|                null|   CC BY-SA 2.5|2011-01-31 21:09:...|          null|  8|2011-01-31 21:09:...|                null|                  null|             null|             null|          29|        1|          2|     5|                null|                null|      null|\n|               16|           4|<p>It seems that ...|                null|            4|                null|   CC BY-SA 2.5|2011-01-31 21:09:...|          null|  9|2019-11-23 17:55:...|                null|                  null|             null|             null|          37|     null|          1|    17|<equities><vix><h...|Hedging stocks wi...|      2249|\n|             null|        null|<p>Clark,\\nThis i...|                null|            2|2011-01-31 21:10:...|   CC BY-SA 2.5|2011-01-31 21:10:...|          null| 10|2011-01-31 21:10:...|                null|                  null|             null|             null|          43|        1|          2|    20|                null|                null|      null|\n|             null|        null|<p><a href=\"http:...|                null|            0|                null|   CC BY-SA 2.5|2011-01-31 21:10:...|          null| 11|2011-01-31 21:10:...|                null|                  null|             null|             null|          35|        1|          2|     6|                null|                null|      null|\n+-----------------+------------+--------------------+--------------------+-------------+--------------------+---------------+--------------------+--------------+---+--------------------+--------------------+----------------------+-----------------+-----------------+------------+---------+-----------+------+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "quant_raw_path = \"/dbdemos/product/llm/quant/raw\"\n",
    "print(f\"loading raw xml dataset under {quant_raw_path}\")\n",
    "raw_quant = spark.read.format(\"xml\").option(\"rowTag\", \"row\").load(f\"{quant_raw_path}/Posts.xml\")\n",
    "raw_quant.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0deafa10-e720-4933-8005-84779009d547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n| id|                text|parent_id|\n+---+--------------------+---------+\n|  1|To get the ball r...|     null|\n|  2|I like Statistics...|        1|\n|  3|I want to start l...|     null|\n|  4|John C. Hull's \"O...|        1|\n|  5|How do you model ...|     null|\n|  6|This may be too b...|        1|\n|  8|I like the follow...|        1|\n|  9|It seems that VIX...|     null|\n| 10|Clark,\\nThis is o...|        1|\n| 11|Options, Futures,...|        1|\n+---+--------------------+---------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf\n",
    "\n",
    "#UDF to transform html content as text\n",
    "@pandas_udf(\"string\")\n",
    "def html_to_text(html):\n",
    "  return html.apply(lambda x: BeautifulSoup(x).get_text())\n",
    "\n",
    "quant_df =(raw_quant\n",
    "                  .filter(\"_Score >= 5\") # keep only good answer/question\n",
    "                  .filter(length(\"_Body\") <= 1000) #remove too long questions\n",
    "                  .withColumn(\"text\", html_to_text(\"_Body\")) #Convert html to text\n",
    "                  .withColumnsRenamed({\"_Id\": \"id\", \"_ParentId\": \"parent_id\"})\n",
    "                  .select(\"id\", \"text\", \"parent_id\"))\n",
    "\n",
    "quant_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb645e64-bcd0-45dd-a038-7a7a3ff035f9",
     "showTitle": true,
     "title": "Get 10 longest answers"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n|                text|text_length|\n+--------------------+-----------+\n|Co-integration is...|        986|\n|Well, what you fi...|        983|\n|VIX is calculated...|        980|\n|The answer to you...|        977|\n|The estimation of...|        975|\n|Recently I've rea...|        970|\n|According to my u...|        967|\n|Buy copies of Bre...|        966|\n|There have been n...|        965|\n|Assuming you avoi...|        963|\n+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "docs_df = quant_df.withColumn('text_length', length(col('text')))\\\n",
    "                    .orderBy(col('text_length').desc()).limit(10)\\\n",
    "                    .select('text','text_length')\n",
    "docs_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1eb06d3-cb2d-4764-9b1a-345f68801524",
     "showTitle": true,
     "title": "Summarizing the documents"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>text</th><th>text_length</th><th>summary</th><th>summary_length</th></tr></thead><tbody><tr><td>Co-integration is a measure / indicator of the long running relationship between 2 or more time series.\n",
       " \n",
       "A short answer to how you can use it, is the pairs trading strategy or in Econometrics can be used to formulate a regression.\n",
       " \n",
       "using the classic example, you can use 2 stocks like Coke (C) and Pepsi (P) (or commodities such as Gold and Silver) in a pairs trading strategy. Your strategy will involve first finding out if the stocks are co-integrated; if they are then you will need to have a strat like:\n",
       " \n",
       "- If the spread (C - P) > threshold then sell C and buy P\n",
       "- If the spread (C - P) < threshold then buy C and sell P\n",
       " \n",
       "The idea here is that if the spread widens say C increases then eventually P will also increase or C will eventually revert to some long running value.\n",
       " \n",
       "The key challenge here is to determine when the spread is at its optimal value, so that you know when to enter / exit the trade\n",
       " \n",
       "Very quick and basic, there are tons of info on this strat on the web.\n",
       "</td><td>986</td><td> Co-integration is a measure / indicator of the long running relationship between 2 or more time series . The idea here is that if the spread widens say C increases then eventually P will also increase or C will eventually revert to some long running value . The key challenge is to determine when the spread is at its optimal value .</td><td>334</td></tr><tr><td>Well, what you find is that the introduction of stochastic vol changes the delta of your options.  So what does this mean? If the new delta reduces the variance of your hedged portfolio versus the pure local vol model , then it means that the introduction of stochastic vol has resulted in a better description of market dynamics versus the pure local vol model.  \n",
       "Secondly, what you also find is that you can have different models all of which reprice the vanilla options, but that some exotic options have very different prices in the different models.  For example , the introduction of stochastic vol can be done in a way that preserves the vanilla option prices , but it lowers the value of forward implied volatilities in the model versus a simple local vol model. Thus, exotics that depend on forward vols ( cliques, Bermudan etc) are priced very differently.  Hence another reason to introduce stochastic vol is to improve the pricing of exotics, given the vanilla market.  \n",
       "</td><td>983</td><td> Stochastic vol has resulted in a better description of market dynamics versus the pure local vol model . Exotics that depend on forward vols ( cliques, Bermudan etc) are priced very differently . Another reason to introduce stochastic vol is to improve the pricing of exotics, given the vanilla market .</td><td>304</td></tr><tr><td>VIX is calculated from a basket of SPX options, and VIX futures expire into following expiration, e.g. September VIX futures that will expire next Wednesday will use SPX October options chain to calculate settlement value. If $B$ is the value of the basket then VIX value at expiration is $\\sqrt{ B }$. Then VIX futures price is the expectation of the basket $VIX _{F} = E[\\sqrt{ B }]$. Delta of the VIX futures price with respect to the basket would be $$\\ \\frac{\\partial VIX _{F}}{\\partial B} = \\frac{\\partial E[\\sqrt{ B }]}{\\partial B}$$\n",
       "As you can see that taking that expectation is not simple, since there is no simple connection  between VIX futures greeks and SPX options greeks because of the expectation and square root. So \"use the chain rule and linearity of the derivative\" approach would not get you anywhere. But that does not mean that such derivative is 0. Such derivative can be calculated in Malliavin sense, but that is probably not what you're looking for.  \n",
       "</td><td>980</td><td> VIX is calculated from a basket of SPX options, and VIX futures expire into following expiration . If $B$ is the value of the basket then VIX value at expiration is $\\sqrt{ B }$. If VIX price is the expectation of basket $VIX _{F} = E[\\sqrt { B }]$. Delta of the Vix futures price with respect to the basket would be$$\\ \\frac{\\partial VIX _ {F]{\\partial B} = \\frac{{ B}$$</td><td>372</td></tr><tr><td>The answer to your first four questions is affirmative.  Option-adjusting the spread makes an equivalence between everything theoretically possible, but the quality of results depends significantly on the quality of your interest rate model and its calibration.  My personal opinion, though, is that the results need to be treated carefully because the OAS model does not (typically) include stochastic credit spreads and potential capital structure changes, and therefore tends to underprice the embedded options.\n",
       "For a bond with a single call date, Delta would be the risk-neutral exercise probability, but that situation is nearly nonexistent.  Since the interest rate model used for OAS can easily compute the exercise probability alongside valuation, you should just use the model to get it.\n",
       "If you are not computing OAS yourself, you are probably working with pretty pathetic numbers because most commercial sources are poorly calibrated (I'm looking at you, Bloomberg).\n",
       "</td><td>977</td><td> Option-adjusting the spread makes an equivalence between everything theoretically possible . The quality of the model depends significantly on the quality of your model and its calibration . The OAS model does not (typically) include credit spreads and potential capital structure changes and therefore tends to underprice the embedded options .</td><td>346</td></tr><tr><td>The estimation of a covariance matrix is unstable unless the number of historical observations $T$ is greater than the number of securities $N$ (5000 in your example). Consider that 10 years of data represents only 120 monthly observations and about 2500 daily observations.\n",
       "Depending on the application, using data dating farther back than 10 years may be impractical and undesirable for many reasons -- de-listed stocks, regime changes, etc.  In fact, risk management applications often require covariance estimations over recent periods of time (1-3 years).\n",
       "Computational applications ranging from portfolio construction to Monte Carlo simulation generally require that the estimated covariance matrix is non-singular and positive definite. \n",
       "If N is greater than T, then the estimated covariance matrix will be singular. Furthermore a variety of small sample problems persist until the number of observations is an order of magnitude larger than the number of securities.\n",
       "</td><td>975</td><td> The estimation of a covariance matrix is unstable unless the number of historical observations $T$ is greater than number of securities $N$ (5000 in your example) Consider that 10 years of data represents only 120 monthly observations and about 2500 daily observations . Risk management applications often require covariance estimations over recent periods of time .</td><td>367</td></tr><tr><td>Recently I've read some books about quantative approach to fundamental investing:\n",
       "- What works on Wall Street - James O'Shaughnessy\n",
       "- Quantitative Value - Wesley Gray, Tobias Carlisle\n",
       "- Quantitative Strategies - Richard Tortoriello\n",
       "Basically, their research methodology, can be summarized as, we have a set of indicators:\n",
       "- value (E/P, EBIT/TEV, S/P, ...)\n",
       "- momentum (RSI, ...)\n",
       "- quality (Piotroski score,...)\n",
       "- growth (PEG, ...)\n",
       "We rank stocks and assign to deciles. We decide how often we rebalance portfolio (rather low frequency) and which strategy to apply. We calculate return,cagr, sharpe etc. for every decile/strategy.\n",
       "I'm looking for free/open-source framework/library to reproduce similar research. I can't use yahoo data (non-yahoo stock exchange), so I need to load my own data. I consider to use python pandas for this, but maybe a better solution exists. Unfortunately, I've only found libraries for pair trading and technical analysis for single stock. \n",
       "</td><td>970</td><td> Recently I've read some books about quantative approach to fundamental investing . I'm looking for free/open-source framework/library to reproduce similar research . I can't use yahoo data (non-yahoo stock exchange) so I need to load my own data . I consider to use python pandas for this, but maybe a better solution exists .</td><td>327</td></tr><tr><td>According to my understanding, synthetic CDOs are essentially credit default swaps (CDS) for a bunch of loans, stored in a special purpose vehicle (SPV). Here, the investor (the one who buys the synthetic CDO) is essentially buying insurance against defaults of loans that the investor doesn't hold. Therefore, if the loan defaults the investor gets paid by the issuer of the CDSs, and until that happens the investor pays a premium to the issuer of the CDSs (somehow?).\n",
       "My understanding is mostly based on the assumption that 1) buying synthetic CDO (i.e. being an investor) means betting against the loans, 2) synthetic CDOs don't contain any loans.\n",
       "I wouldn't be surprised if I'm wrong, so please correct me where I'm wrong. In particular, it doesn't make sense to me why is there even CDO in the name of this synthetic CDO, as it is just a basket of CDSs and there are no loans in it. Also, how can the investor payout the premium to the issuer if it is bundled?\n",
       "</td><td>967</td><td> Synthetic CDOs are essentially credit default swaps (CDS) for a bunch of loans stored in a special purpose vehicle . Here, the investor (the one who buys the synthetic CDO) is essentially buying insurance against defaults of loans that the investor doesn't hold . If the loan defaults the investor gets paid by the issuer of the CDSs .</td><td>336</td></tr><tr><td>Buy copies of Brent Oksendal's \"Stochastic Differential Equations An Introduction with Applications\" and Thomas Bjork's \"Arbitrage Theory in Continuous Time.\" These are well written graduate level textbooks. I can't promise it will be painless, but if you want to understand continuous time derivative pricing models these are a place to start. \n",
       "Another option is to not worry about continuous time models and get a copy of Stanley R. Pliska's \"Introduction to Mathematical Finance.\" It is a graduate textbook covering discrete time models. To use these models all you need to know is linear algebra and how to optimize linear equations using the simplex method. (not to be confused with the simplex numerical optimization algorithm.) \n",
       "Bluntly put: Ito Integration can be viewed two ways.\n",
       "1) As an incomplete Riemann Stieltjes Integral\n",
       "2) An extended Lebesgue Integral. \n",
       "If you have no idea what either of the above two things are, go with the descrete time models.\n",
       "</td><td>966</td><td> Buy copies of Brent Oksendal's \"Stochastic Differential Equations An Introduction with Applications\" and Thomas Bjork's \"Arbitrage Theory in Continuous Time\" These are well written graduate level textbooks . To use these models all you need to know is linear algebra and how to optimize linear equations using the simplex method .</td><td>331</td></tr><tr><td>There have been numerous exotic trading desk blow ups lately, related to various reasons. However, in particular, one bank had some issues where they were pricing autocallable notes with Local Volatility and not producing a Delta \"true up\" using Stochastic Volatility that is common among other banks. In other words, Delta of the autocallable notes is higher in magnitude under Local Volatility compared to Stochastic Volatility. Since the bank thought it was holding more (negative) Delta as a result of the Local Volatility model, they bought too much stock to hedge and had large losses when the market declined.\n",
       "Can someone provide an intuitive explanation of why Delta is higher in autocallable products under Local Volatility compared to Stochastic Volatility? The price of the product is different under the two volatility models on account of vol-of-vol differences, but it's not entirely clear to me why the Delta difference is in this direction.\n",
       "Thanks.\n",
       "</td><td>965</td><td> Bank had some issues where they were pricing autocallable notes with Local Volatility and not producing a Delta \"true up\" using Stochastic Volatility . Bank thought it was holding more (negative) Delta as a result of the local Volatility model, they bought too much stock to hedge and had large losses when the market declined .</td><td>329</td></tr><tr><td>Assuming you avoid data-snooping bias and all the potential pitfalls of using the past to predict the future, trusting genetic algorithms to find the \"right\" solution pretty much boils down to the same bet you make when you actively manage a portfolio, whether quantitatively or discretionary. If you believe in market efficiency then increasing your transaction costs from active management is illogical. If, however you believe there are structural & psychological patterns or \"flaws\" to be exploited and the payoff is worth the time and money for researching and implementing a strategy the logical choice is active management.\n",
       "Running a GA derived strategy is an implicit bet against market efficiency. You're basically saying \"I think there are mis-valuations that occur from some reason\" (masses of irrational people, mutual funds herding because of mis-aligned incentives, etc.) and \"running this GA can sort this mass of data out way quicker than I can.\"\n",
       "</td><td>963</td><td> Running a GA derived strategy is an implicit bet against market efficiency . You're basically saying \"I think there are mis-valuations that occur from some reason\" (masses of irrational people, mutual funds herding because of mis-aligned incentives) and \"running this GA can sort this mass of data out way quicker than I can\"</td><td>326</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Co-integration is a measure / indicator of the long running relationship between 2 or more time series.\n \nA short answer to how you can use it, is the pairs trading strategy or in Econometrics can be used to formulate a regression.\n \nusing the classic example, you can use 2 stocks like Coke (C) and Pepsi (P) (or commodities such as Gold and Silver) in a pairs trading strategy. Your strategy will involve first finding out if the stocks are co-integrated; if they are then you will need to have a strat like:\n \n- If the spread (C - P) > threshold then sell C and buy P\n- If the spread (C - P) < threshold then buy C and sell P\n \nThe idea here is that if the spread widens say C increases then eventually P will also increase or C will eventually revert to some long running value.\n \nThe key challenge here is to determine when the spread is at its optimal value, so that you know when to enter / exit the trade\n \nVery quick and basic, there are tons of info on this strat on the web.\n",
         986,
         " Co-integration is a measure / indicator of the long running relationship between 2 or more time series . The idea here is that if the spread widens say C increases then eventually P will also increase or C will eventually revert to some long running value . The key challenge is to determine when the spread is at its optimal value .",
         334
        ],
        [
         "Well, what you find is that the introduction of stochastic vol changes the delta of your options.  So what does this mean? If the new delta reduces the variance of your hedged portfolio versus the pure local vol model , then it means that the introduction of stochastic vol has resulted in a better description of market dynamics versus the pure local vol model.  \nSecondly, what you also find is that you can have different models all of which reprice the vanilla options, but that some exotic options have very different prices in the different models.  For example , the introduction of stochastic vol can be done in a way that preserves the vanilla option prices , but it lowers the value of forward implied volatilities in the model versus a simple local vol model. Thus, exotics that depend on forward vols ( cliques, Bermudan etc) are priced very differently.  Hence another reason to introduce stochastic vol is to improve the pricing of exotics, given the vanilla market.  \n",
         983,
         " Stochastic vol has resulted in a better description of market dynamics versus the pure local vol model . Exotics that depend on forward vols ( cliques, Bermudan etc) are priced very differently . Another reason to introduce stochastic vol is to improve the pricing of exotics, given the vanilla market .",
         304
        ],
        [
         "VIX is calculated from a basket of SPX options, and VIX futures expire into following expiration, e.g. September VIX futures that will expire next Wednesday will use SPX October options chain to calculate settlement value. If $B$ is the value of the basket then VIX value at expiration is $\\sqrt{ B }$. Then VIX futures price is the expectation of the basket $VIX _{F} = E[\\sqrt{ B }]$. Delta of the VIX futures price with respect to the basket would be $$\\ \\frac{\\partial VIX _{F}}{\\partial B} = \\frac{\\partial E[\\sqrt{ B }]}{\\partial B}$$\nAs you can see that taking that expectation is not simple, since there is no simple connection  between VIX futures greeks and SPX options greeks because of the expectation and square root. So \"use the chain rule and linearity of the derivative\" approach would not get you anywhere. But that does not mean that such derivative is 0. Such derivative can be calculated in Malliavin sense, but that is probably not what you're looking for.  \n",
         980,
         " VIX is calculated from a basket of SPX options, and VIX futures expire into following expiration . If $B$ is the value of the basket then VIX value at expiration is $\\sqrt{ B }$. If VIX price is the expectation of basket $VIX _{F} = E[\\sqrt { B }]$. Delta of the Vix futures price with respect to the basket would be$$\\ \\frac{\\partial VIX _ {F]{\\partial B} = \\frac{{ B}$$",
         372
        ],
        [
         "The answer to your first four questions is affirmative.  Option-adjusting the spread makes an equivalence between everything theoretically possible, but the quality of results depends significantly on the quality of your interest rate model and its calibration.  My personal opinion, though, is that the results need to be treated carefully because the OAS model does not (typically) include stochastic credit spreads and potential capital structure changes, and therefore tends to underprice the embedded options.\nFor a bond with a single call date, Delta would be the risk-neutral exercise probability, but that situation is nearly nonexistent.  Since the interest rate model used for OAS can easily compute the exercise probability alongside valuation, you should just use the model to get it.\nIf you are not computing OAS yourself, you are probably working with pretty pathetic numbers because most commercial sources are poorly calibrated (I'm looking at you, Bloomberg).\n",
         977,
         " Option-adjusting the spread makes an equivalence between everything theoretically possible . The quality of the model depends significantly on the quality of your model and its calibration . The OAS model does not (typically) include credit spreads and potential capital structure changes and therefore tends to underprice the embedded options .",
         346
        ],
        [
         "The estimation of a covariance matrix is unstable unless the number of historical observations $T$ is greater than the number of securities $N$ (5000 in your example). Consider that 10 years of data represents only 120 monthly observations and about 2500 daily observations.\nDepending on the application, using data dating farther back than 10 years may be impractical and undesirable for many reasons -- de-listed stocks, regime changes, etc.  In fact, risk management applications often require covariance estimations over recent periods of time (1-3 years).\nComputational applications ranging from portfolio construction to Monte Carlo simulation generally require that the estimated covariance matrix is non-singular and positive definite. \nIf N is greater than T, then the estimated covariance matrix will be singular. Furthermore a variety of small sample problems persist until the number of observations is an order of magnitude larger than the number of securities.\n",
         975,
         " The estimation of a covariance matrix is unstable unless the number of historical observations $T$ is greater than number of securities $N$ (5000 in your example) Consider that 10 years of data represents only 120 monthly observations and about 2500 daily observations . Risk management applications often require covariance estimations over recent periods of time .",
         367
        ],
        [
         "Recently I've read some books about quantative approach to fundamental investing:\n- What works on Wall Street - James O'Shaughnessy\n- Quantitative Value - Wesley Gray, Tobias Carlisle\n- Quantitative Strategies - Richard Tortoriello\nBasically, their research methodology, can be summarized as, we have a set of indicators:\n- value (E/P, EBIT/TEV, S/P, ...)\n- momentum (RSI, ...)\n- quality (Piotroski score,...)\n- growth (PEG, ...)\nWe rank stocks and assign to deciles. We decide how often we rebalance portfolio (rather low frequency) and which strategy to apply. We calculate return,cagr, sharpe etc. for every decile/strategy.\nI'm looking for free/open-source framework/library to reproduce similar research. I can't use yahoo data (non-yahoo stock exchange), so I need to load my own data. I consider to use python pandas for this, but maybe a better solution exists. Unfortunately, I've only found libraries for pair trading and technical analysis for single stock. \n",
         970,
         " Recently I've read some books about quantative approach to fundamental investing . I'm looking for free/open-source framework/library to reproduce similar research . I can't use yahoo data (non-yahoo stock exchange) so I need to load my own data . I consider to use python pandas for this, but maybe a better solution exists .",
         327
        ],
        [
         "According to my understanding, synthetic CDOs are essentially credit default swaps (CDS) for a bunch of loans, stored in a special purpose vehicle (SPV). Here, the investor (the one who buys the synthetic CDO) is essentially buying insurance against defaults of loans that the investor doesn't hold. Therefore, if the loan defaults the investor gets paid by the issuer of the CDSs, and until that happens the investor pays a premium to the issuer of the CDSs (somehow?).\nMy understanding is mostly based on the assumption that 1) buying synthetic CDO (i.e. being an investor) means betting against the loans, 2) synthetic CDOs don't contain any loans.\nI wouldn't be surprised if I'm wrong, so please correct me where I'm wrong. In particular, it doesn't make sense to me why is there even CDO in the name of this synthetic CDO, as it is just a basket of CDSs and there are no loans in it. Also, how can the investor payout the premium to the issuer if it is bundled?\n",
         967,
         " Synthetic CDOs are essentially credit default swaps (CDS) for a bunch of loans stored in a special purpose vehicle . Here, the investor (the one who buys the synthetic CDO) is essentially buying insurance against defaults of loans that the investor doesn't hold . If the loan defaults the investor gets paid by the issuer of the CDSs .",
         336
        ],
        [
         "Buy copies of Brent Oksendal's \"Stochastic Differential Equations An Introduction with Applications\" and Thomas Bjork's \"Arbitrage Theory in Continuous Time.\" These are well written graduate level textbooks. I can't promise it will be painless, but if you want to understand continuous time derivative pricing models these are a place to start. \nAnother option is to not worry about continuous time models and get a copy of Stanley R. Pliska's \"Introduction to Mathematical Finance.\" It is a graduate textbook covering discrete time models. To use these models all you need to know is linear algebra and how to optimize linear equations using the simplex method. (not to be confused with the simplex numerical optimization algorithm.) \nBluntly put: Ito Integration can be viewed two ways.\n1) As an incomplete Riemann Stieltjes Integral\n2) An extended Lebesgue Integral. \nIf you have no idea what either of the above two things are, go with the descrete time models.\n",
         966,
         " Buy copies of Brent Oksendal's \"Stochastic Differential Equations An Introduction with Applications\" and Thomas Bjork's \"Arbitrage Theory in Continuous Time\" These are well written graduate level textbooks . To use these models all you need to know is linear algebra and how to optimize linear equations using the simplex method .",
         331
        ],
        [
         "There have been numerous exotic trading desk blow ups lately, related to various reasons. However, in particular, one bank had some issues where they were pricing autocallable notes with Local Volatility and not producing a Delta \"true up\" using Stochastic Volatility that is common among other banks. In other words, Delta of the autocallable notes is higher in magnitude under Local Volatility compared to Stochastic Volatility. Since the bank thought it was holding more (negative) Delta as a result of the Local Volatility model, they bought too much stock to hedge and had large losses when the market declined.\nCan someone provide an intuitive explanation of why Delta is higher in autocallable products under Local Volatility compared to Stochastic Volatility? The price of the product is different under the two volatility models on account of vol-of-vol differences, but it's not entirely clear to me why the Delta difference is in this direction.\nThanks.\n",
         965,
         " Bank had some issues where they were pricing autocallable notes with Local Volatility and not producing a Delta \"true up\" using Stochastic Volatility . Bank thought it was holding more (negative) Delta as a result of the local Volatility model, they bought too much stock to hedge and had large losses when the market declined .",
         329
        ],
        [
         "Assuming you avoid data-snooping bias and all the potential pitfalls of using the past to predict the future, trusting genetic algorithms to find the \"right\" solution pretty much boils down to the same bet you make when you actively manage a portfolio, whether quantitatively or discretionary. If you believe in market efficiency then increasing your transaction costs from active management is illogical. If, however you believe there are structural & psychological patterns or \"flaws\" to be exploited and the payoff is worth the time and money for researching and implementing a strategy the logical choice is active management.\nRunning a GA derived strategy is an implicit bet against market efficiency. You're basically saying \"I think there are mis-valuations that occur from some reason\" (masses of irrational people, mutual funds herding because of mis-aligned incentives, etc.) and \"running this GA can sort this mass of data out way quicker than I can.\"\n",
         963,
         " Running a GA derived strategy is an implicit bet against market efficiency . You're basically saying \"I think there are mis-valuations that occur from some reason\" (masses of irrational people, mutual funds herding because of mis-aligned incentives) and \"running this GA can sort this mass of data out way quicker than I can\"",
         326
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "text_length",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "summary_length",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "#Make sure we clean the memory\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    from numba import cuda\n",
    "    cuda.get_current_device().reset()\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't clean the memory: {e}\")\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def summarize(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Load the model for summarization\n",
    "    torch.cuda.empty_cache()\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=\"cuda:0\")\n",
    "    def summarize_txt(text):\n",
    "      return summarizer(text)[0]['summary_text']\n",
    "\n",
    "    for serie in iterator:\n",
    "        # get a summary for each row\n",
    "        yield serie.apply(summarize_txt)\n",
    "\n",
    "docs_df = docs_df.repartition(1)\\\n",
    "                 .withColumn(\"summary\", summarize(\"text\"))\\\n",
    "                 .withColumn('summary_length', length(col('summary')))\n",
    "display(docs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02307b85-f394-40ec-b189-c817d72e165e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 877308197622273,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "Summarization_using_distilbart-cnn-12-6",
   "widgets": {
    "reset_vector_database": {
     "currentValue": "false",
     "nuid": "69d7b530-c330-43e4-8869-96a9682f9908",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Recompute embeddings for chromadb",
      "name": "reset_vector_database",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "false",
        "true"
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
